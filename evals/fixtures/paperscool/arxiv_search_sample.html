<!doctype html>
<html>
  <head><title>kv cache acceleration | Cool Papers</title></head>
  <body>
    <div class="panel paper" id="2412.19442" keywords="cache,management,llms,acceleration">
      <h2 class="title">
        <a href="https://arxiv.org/abs/2412.19442" target="_blank" title="1/1000"><span>#1</span></a>
        <a class="title-link notranslate" href="/arxiv/2412.19442" id="title-2412.19442" target="_blank">
          A Survey on Large Language Model Acceleration based on KV Cache Management
        </a>
        <a class="title-pdf notranslate" data="https://arxiv.org/pdf/2412.19442" id="pdf-2412.19442">[PDF<sup id="pdf-stars-2412.19442">24</sup>]</a>
        <a class="title-kimi notranslate" id="kimi-2412.19442">[Kimi<sup id="kimi-stars-2412.19442">27</sup>]</a>
      </h2>
      <p class="metainfo authors notranslate" id="authors-2412.19442"><strong>Authors</strong>:
        <a class="author notranslate" href="https://arxiv.org/search/?searchtype=author&amp;query=Haoyang+Li">Haoyang Li</a>,
        <a class="author notranslate" href="https://arxiv.org/search/?searchtype=author&amp;query=Yiming+Li">Yiming Li</a>
      </p>
      <p class="metainfo subjects"><strong>Subject</strong>: Computation and Language</p>
      <p class="metainfo date"><strong>Publish</strong>: 2024-12-15 03:21:00 UTC</p>
      <p class="summary">Large Language Models (LLMs) have revolutionized many domains. We review KV cache optimization methods for low-latency inference.</p>
    </div>

    <div class="panel paper" id="2505.17787" keywords="cache,quantization,llm,acceleration">
      <h2 class="title">
        <a href="https://arxiv.org/abs/2505.17787" target="_blank" title="2/1000"><span>#2</span></a>
        <a class="title-link notranslate" href="/arxiv/2505.17787" id="title-2505.17787" target="_blank">
          Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM Acceleration
        </a>
        <a class="title-pdf notranslate" data="https://arxiv.org/pdf/2505.17787" id="pdf-2505.17787">[PDF<sup id="pdf-stars-2505.17787">15</sup>]</a>
        <a class="title-kimi notranslate" id="kimi-2505.17787">[Kimi<sup id="kimi-stars-2505.17787">18</sup>]</a>
      </h2>
      <p class="metainfo authors notranslate" id="authors-2505.17787"><strong>Authors</strong>:
        <a class="author notranslate" href="https://arxiv.org/search/?searchtype=author&amp;query=Peilin+Chen">Peilin Chen</a>,
        <a class="author notranslate" href="https://arxiv.org/search/?searchtype=author&amp;query=Xiaoxuan+Yang">Xiaoxuan Yang</a>
      </p>
      <p class="metainfo subjects"><strong>Subject</strong>: Machine Learning</p>
      <p class="metainfo date"><strong>Publish</strong>: 2025-05-23 10:12:00 UTC</p>
      <p class="summary">We propose on-the-fly KV cache pruning and quantization to improve decoding throughput while preserving quality.</p>
    </div>
  </body>
</html>
